![Docker](https://img.shields.io/badge/Docker-Ready-blue)
![Spark](https://img.shields.io/badge/Spark-Streaming-orange)
![Kafka](https://img.shields.io/badge/Kafka-Realtime-black)

#  Real-Time Analytics Pipeline (Kafka â†’ Spark Streaming)

**Why I built this project ?**

I didnâ€™t build this project just to â€œ use **Kafka** and **Spark**â€.

I built it because real-world data doesnâ€™t wait.

Transactions, clicks, payments, and events are constantly flowing and i wanted to understand what really happens from the moment an event is created to the moment it is processed and analyzed in real time.

So I decided to build the whole pipeline myself, end to end.


# :lotus_position: You can see the Full Project via Figma (easier) ! 
Click this link to view the Project ->  https://www.figma.com/make/v0DRrtrvwKEjx9iA1PZjNa/Confetti-Explosion-on-Click?p=f&t=avk3mMhhpyjcGvNg-0&fullscreen=1





---

## Architecture Overview

```
Producer (Python)
      â†“
   Kafka Topic (transactions)
      â†“
Spark Structured Streaming
      â†“
 Console Output / (Next: Parquet, MinIO, Dashboard)
```

---

## Tech Stack

* **Python 3.10+**
* **Apache Kafka** (event streaming)
* **Apache Spark 3.5.1** (Structured Streaming)
* **Docker & Docker Compose**
* **Zookeeper** (Kafka dependency)

---

## ğŸ“ Project Structure

```
realtime-analytics-pipeline/
â”‚
â”œâ”€â”€ producer/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ producer.py          # Kafka producer (simulated transactions)
â”‚
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ docker-compose.yml   # Kafka + Spark cluster
â”‚   â””â”€â”€ streaming_test.py    # Spark Structured Streaming job
â”‚
â”œâ”€â”€ data/                    # (future) persisted streaming outputs
â”œâ”€â”€ dashboard/               # (future) Streamlit dashboard
â””â”€â”€ README.md
```

---

## Data Flow

Each event represents a transaction:

```json
{
  "transaction_id": "24236",
  "amount": 472.32,
  "city": "Nice",
  "timestamp": 1765741726.39
}
```

Flow:

1. Python producer sends events to Kafka
2. Spark reads Kafka topic in real time
3. JSON is parsed with an explicit schema
4. Data is streamed to the console (live)

---

## â–¶ï¸ How to Run the Project

### 1ï¸âƒ£ Start Kafka & Spark Cluster

```bash
docker compose -f spark/docker-compose.yml up -d
```

Verify:

* Spark Master UI â†’ [http://localhost:8080](http://localhost:8080)
* Spark Worker UI â†’ [http://localhost:8081](http://localhost:8081)

---

### 2ï¸âƒ£ Create Kafka Topic

```bash
docker exec -it kafka kafka-topics \
  --bootstrap-server kafka:9092 \
  --create \
  --topic transactions \
  --partitions 1 \
  --replication-factor 1
```

---

### 3ï¸âƒ£ Start the Kafka Producer

```bash
docker compose -f spark/docker-compose.yml up -d --build producer
```

Check producer logs:

```bash
docker logs -f producer
```

---

### 4ï¸âƒ£ Run Spark Streaming Job

```bash
docker exec -it spark-master \
  /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  /opt/spark/work-dir/streaming_test.py
```

You should see **live streaming data** in the terminal.

---

## ğŸ“Š Spark UI

* Spark Master: [http://localhost:8080](http://localhost:8080)
* Spark Worker: [http://localhost:8081](http://localhost:8081)

You can observe:

* Running applications
* Executors
* Memory & CPU usage

---

## ğŸ§  Key Concepts Demonstrated

* Event-driven architecture
* Kafka topics & producers
* Spark Structured Streaming
* JSON schema enforcement
* Dockerized data platforms
* Real-time processing semantics

---

## ğŸš§ Next Improvements (Roadmap)

* â± Windowed aggregations (KPIs per minute)
* ğŸ’¾ Persist streaming output to Parquet / MinIO
* ğŸ“ˆ Streamlit real-time dashboard
* â˜ï¸ Cloud deployment (AWS / GCP)
* ğŸ§ª Data quality checks

---

## ğŸ¯ Why This Project Matters

This project reflects **real-world data engineering pipelines** used in:

* FinTech
* E-commerce
* IoT & event analytics

It demonstrates practical skills expected from a **Data Engineer / AI Engineer intern**.

---

## ğŸ‘¤ Author

**Wassim Elmoufakkir**
MSc Data Engineering for Artificial Intelligence


---

â­ If you find this project useful, feel free to star the repository!

